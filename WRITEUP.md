# Short Write-Up (200–300 words)

This browser-based LLM Agent Proof-of-Concept demonstrates a small, iterative reasoning loop using OpenAI-style function calling. The agent accepts user input, sends the conversation to the LLM with a list of available function signatures, and then executes any tool calls the model requests. Each tool execution (search, aipipe, run_js) is sent back to the model as a `function` role result so the LLM can incorporate live tool outputs into its next step. The loop continues until the LLM returns a final assistant message without requesting additional tools.

For web search, the agent supports SerpAPI (if a key is provided) and falls back to the DuckDuckGo Instant Answer API. Search results are converted into short snippets and returned as structured JSON to the LLM. The AI Pipe tool is implemented as a simple POST to a user-specified endpoint that returns JSON or text; this lets instructors demonstrate chained AI workflows via an external proxy. The run_js tool executes JavaScript inside a sandboxed iframe (using `srcdoc`) so the created POC can safely evaluate code and capture logs or returned values. Each tool result is added as a `function` message, preserving the OpenAI tool-calling pattern and enabling multi-step reasoning.

This POC emphasizes clarity and hackability: all code is contained in `index.html` and the agent's tool functions are easy to extend. Security caveats are noted — production systems should move API keys and tool execution to trusted server-side components rather than running them directly in the browser.
